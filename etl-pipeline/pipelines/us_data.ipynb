{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import covid_etl as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input Dir: /Users/adammcquistan/code/ambassador/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports_us\n"
     ]
    }
   ],
   "source": [
    "INPUT_DATA_DIR = os.path.join(os.path.abspath('../../../'),\n",
    "                        'COVID-19',\n",
    "                        'csse_covid_19_data',\n",
    "                        'csse_covid_19_daily_reports_us')\n",
    "print(\"Input Dir: \" + INPUT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output Dir: /Users/adammcquistan/code/ambassador/COVID-19-TRANSFORMED\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMED_DATA_DIR = os.path.join(os.path.abspath('../../../'), 'COVID-19-TRANSFORMED')\n",
    "\n",
    "if not os.path.exists(TRANSFORMED_DATA_DIR):\n",
    "    os.makedirs(TRANSFORMED_DATA_DIR)\n",
    "\n",
    "print(\"Output Dir: \" + TRANSFORMED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix any BOM files (there are some early on ones in Jan 2020, could be more later)\n",
    "\n",
    "input_files = [f for f in os.listdir(INPUT_DATA_DIR) if f.endswith('.csv')]\n",
    "\n",
    "for f in input_files:\n",
    "    input_f = os.path.join(INPUT_DATA_DIR, f)\n",
    "    output_f = os.path.join(TRANSFORMED_DATA_DIR, 'us_'+f)\n",
    "    with open(input_f, mode='r', encoding='utf-8-sig') as fin, open(output_f, mode='w', encoding='utf-8') as fout:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transforming us_06-08-2020.csv\n",
      "Transforming us_08-11-2020.csv\n",
      "Transforming us_08-10-2020.csv\n",
      "Transforming us_06-09-2020.csv\n",
      "Transforming us_12-01-2020.csv\n",
      "Transforming us_09-13-2020.csv\n",
      "Transforming us_09-12-2020.csv\n",
      "Transforming us_10-30-2020.csv\n",
      "Transforming us_10-31-2020.csv\n",
      "Transforming us_05-05-2020.csv\n",
      "Transforming us_05-04-2020.csv\n",
      "Transforming us_05-31-2020.csv\n",
      "Transforming us_05-30-2020.csv\n",
      "Transforming us_06-02-2020.csv\n",
      "Transforming us_06-03-2020.csv\n",
      "Transforming us_10-04-2020.csv\n",
      "Transforming us_10-05-2020.csv\n",
      "Transforming us_09-27-2020.csv\n",
      "Transforming us_09-26-2020.csv\n",
      "Transforming us_08-25-2020.csv\n",
      "Transforming us_08-24-2020.csv\n",
      "Transforming us_11-06-2020.csv\n",
      "Transforming us_11-07-2020.csv\n",
      "Transforming us_09-19-2020.csv\n",
      "Transforming us_09-18-2020.csv\n",
      "Transforming us_07-01-2020.csv\n",
      "Transforming us_08-06-2020.csv\n",
      "Transforming us_08-07-2020.csv\n",
      "Transforming us_11-25-2020.csv\n",
      "Transforming us_11-24-2020.csv\n",
      "Transforming us_07-23-2020.csv\n",
      "Transforming us_07-22-2020.csv\n",
      "Transforming us_10-19-2020.csv\n",
      "Transforming us_10-18-2020.csv\n",
      "Transforming us_05-12-2020.csv\n",
      "Transforming us_05-13-2020.csv\n",
      "Transforming us_06-21-2020.csv\n",
      "Transforming us_06-20-2020.csv\n",
      "Transforming us_10-27-2020.csv\n",
      "Transforming us_10-26-2020.csv\n",
      "Transforming us_09-04-2020.csv\n",
      "Transforming us_09-05-2020.csv\n",
      "Transforming us_07-29-2020.csv\n",
      "Transforming us_09-30-2020.csv\n",
      "Transforming us_07-28-2020.csv\n",
      "Transforming us_10-13-2020.csv\n",
      "Transforming us_10-12-2020.csv\n",
      "Transforming us_05-26-2020.csv\n",
      "Transforming us_05-27-2020.csv\n",
      "Transforming us_06-15-2020.csv\n",
      "Transforming us_06-14-2020.csv\n",
      "Transforming us_04-24-2020.csv\n",
      "Transforming us_04-25-2020.csv\n",
      "Transforming us_07-17-2020.csv\n",
      "Transforming us_07-16-2020.csv\n",
      "Transforming us_11-11-2020.csv\n",
      "Transforming us_11-10-2020.csv\n",
      "Transforming us_05-18-2020.csv\n",
      "Transforming us_05-19-2020.csv\n",
      "Transforming us_04-17-2020.csv\n",
      "Transforming us_04-16-2020.csv\n",
      "Transforming us_07-24-2020.csv\n",
      "Transforming us_07-25-2020.csv\n",
      "Transforming us_08-01-2020.csv\n",
      "Transforming us_06-18-2020.csv\n",
      "Transforming us_06-19-2020.csv\n",
      "Transforming us_11-22-2020.csv\n",
      "Transforming us_11-23-2020.csv\n",
      "Transforming us_10-20-2020.csv\n",
      "Transforming us_10-21-2020.csv\n",
      "Transforming us_09-03-2020.csv\n",
      "Transforming us_09-02-2020.csv\n",
      "Transforming us_04-29-2020.csv\n",
      "Transforming us_04-28-2020.csv\n",
      "Transforming us_05-15-2020.csv\n",
      "Transforming us_05-14-2020.csv\n",
      "Transforming us_06-26-2020.csv\n",
      "Transforming us_06-27-2020.csv\n",
      "Transforming us_05-21-2020.csv\n",
      "Transforming us_05-20-2020.csv\n",
      "Transforming us_06-12-2020.csv\n",
      "Transforming us_06-13-2020.csv\n",
      "Transforming us_11-28-2020.csv\n",
      "Transforming us_11-29-2020.csv\n",
      "Transforming us_10-14-2020.csv\n",
      "Transforming us_10-15-2020.csv\n",
      "Transforming us_11-16-2020.csv\n",
      "Transforming us_11-17-2020.csv\n",
      "Transforming us_04-23-2020.csv\n",
      "Transforming us_04-22-2020.csv\n",
      "Transforming us_09-09-2020.csv\n",
      "Transforming us_07-10-2020.csv\n",
      "Transforming us_07-11-2020.csv\n",
      "Transforming us_09-08-2020.csv\n",
      "Transforming us_08-16-2020.csv\n",
      "Transforming us_08-17-2020.csv\n",
      "Transforming us_10-09-2020.csv\n",
      "Transforming us_10-08-2020.csv\n",
      "Transforming us_05-02-2020.csv\n",
      "Transforming us_05-03-2020.csv\n",
      "Transforming us_08-28-2020.csv\n",
      "Transforming us_06-30-2020.csv\n",
      "Transforming us_08-29-2020.csv\n",
      "Transforming us_09-14-2020.csv\n",
      "Transforming us_09-15-2020.csv\n",
      "Transforming us_10-03-2020.csv\n",
      "Transforming us_10-02-2020.csv\n",
      "Transforming us_09-20-2020.csv\n",
      "Transforming us_09-21-2020.csv\n",
      "Transforming us_06-05-2020.csv\n",
      "Transforming us_06-04-2020.csv\n",
      "Transforming us_07-07-2020.csv\n",
      "Transforming us_07-06-2020.csv\n",
      "Transforming us_08-22-2020.csv\n",
      "Transforming us_08-23-2020.csv\n",
      "Transforming us_11-01-2020.csv\n",
      "Transforming us_05-08-2020.csv\n",
      "Transforming us_05-09-2020.csv\n",
      "Transforming us_04-19-2020.csv\n",
      "Transforming us_04-18-2020.csv\n",
      "Transforming us_10-10-2020.csv\n",
      "Transforming us_10-11-2020.csv\n",
      "Transforming us_06-16-2020.csv\n",
      "Transforming us_06-17-2020.csv\n",
      "Transforming us_05-25-2020.csv\n",
      "Transforming us_05-24-2020.csv\n",
      "Transforming us_07-14-2020.csv\n",
      "Transforming us_07-15-2020.csv\n",
      "Transforming us_04-27-2020.csv\n",
      "Transforming us_04-26-2020.csv\n",
      "Transforming us_11-12-2020.csv\n",
      "Transforming us_11-13-2020.csv\n",
      "Transforming us_06-28-2020.csv\n",
      "Transforming us_08-31-2020.csv\n",
      "Transforming us_08-30-2020.csv\n",
      "Transforming us_06-29-2020.csv\n",
      "Transforming us_08-05-2020.csv\n",
      "Transforming us_08-04-2020.csv\n",
      "Transforming us_11-26-2020.csv\n",
      "Transforming us_11-27-2020.csv\n",
      "Transforming us_07-20-2020.csv\n",
      "Transforming us_07-21-2020.csv\n",
      "Transforming us_04-13-2020.csv\n",
      "Transforming us_04-12-2020.csv\n",
      "Transforming us_11-18-2020.csv\n",
      "Transforming us_11-19-2020.csv\n",
      "Transforming us_06-22-2020.csv\n",
      "Transforming us_06-23-2020.csv\n",
      "Transforming us_05-11-2020.csv\n",
      "Transforming us_05-10-2020.csv\n",
      "Transforming us_10-24-2020.csv\n",
      "Transforming us_10-25-2020.csv\n",
      "Transforming us_09-07-2020.csv\n",
      "Transforming us_09-06-2020.csv\n",
      "Transforming us_06-01-2020.csv\n",
      "Transforming us_08-18-2020.csv\n",
      "Transforming us_08-19-2020.csv\n",
      "Transforming us_10-07-2020.csv\n",
      "Transforming us_10-06-2020.csv\n",
      "Transforming us_09-24-2020.csv\n",
      "Transforming us_09-25-2020.csv\n",
      "Transforming us_08-26-2020.csv\n",
      "Transforming us_08-27-2020.csv\n",
      "Transforming us_11-05-2020.csv\n",
      "Transforming us_11-04-2020.csv\n",
      "Transforming us_07-03-2020.csv\n",
      "Transforming us_07-02-2020.csv\n",
      "Transforming us_04-30-2020.csv\n",
      "Transforming us_12-02-2020.csv\n",
      "Transforming us_12-03-2020.csv\n",
      "Transforming us_11-30-2020.csv\n",
      "Transforming us_08-12-2020.csv\n",
      "Transforming us_08-13-2020.csv\n",
      "Transforming us_07-09-2020.csv\n",
      "Transforming us_09-10-2020.csv\n",
      "Transforming us_09-11-2020.csv\n",
      "Transforming us_07-08-2020.csv\n",
      "Transforming us_05-06-2020.csv\n",
      "Transforming us_05-07-2020.csv\n",
      "Transforming us_10-01-2020.csv\n",
      "Transforming us_09-23-2020.csv\n",
      "Transforming us_09-22-2020.csv\n",
      "Transforming us_06-06-2020.csv\n",
      "Transforming us_06-07-2020.csv\n",
      "Transforming us_07-04-2020.csv\n",
      "Transforming us_07-05-2020.csv\n",
      "Transforming us_08-21-2020.csv\n",
      "Transforming us_08-20-2020.csv\n",
      "Transforming us_11-02-2020.csv\n",
      "Transforming us_11-03-2020.csv\n",
      "Transforming us_08-15-2020.csv\n",
      "Transforming us_08-14-2020.csv\n",
      "Transforming us_09-29-2020.csv\n",
      "Transforming us_07-30-2020.csv\n",
      "Transforming us_07-31-2020.csv\n",
      "Transforming us_09-28-2020.csv\n",
      "Transforming us_11-08-2020.csv\n",
      "Transforming us_11-09-2020.csv\n",
      "Transforming us_05-01-2020.csv\n",
      "Transforming us_09-17-2020.csv\n",
      "Transforming us_09-16-2020.csv\n",
      "Transforming us_08-08-2020.csv\n",
      "Transforming us_06-11-2020.csv\n",
      "Transforming us_06-10-2020.csv\n",
      "Transforming us_08-09-2020.csv\n",
      "Transforming us_05-22-2020.csv\n",
      "Transforming us_05-23-2020.csv\n",
      "Transforming us_10-17-2020.csv\n",
      "Transforming us_10-16-2020.csv\n",
      "Transforming us_11-15-2020.csv\n",
      "Transforming us_11-14-2020.csv\n",
      "Transforming us_10-29-2020.csv\n",
      "Transforming us_10-28-2020.csv\n",
      "Transforming us_07-13-2020.csv\n",
      "Transforming us_07-12-2020.csv\n",
      "Transforming us_04-20-2020.csv\n",
      "Transforming us_04-21-2020.csv\n",
      "Transforming us_07-27-2020.csv\n",
      "Transforming us_07-26-2020.csv\n",
      "Transforming us_04-14-2020.csv\n",
      "Transforming us_04-15-2020.csv\n",
      "Transforming us_05-28-2020.csv\n",
      "Transforming us_05-29-2020.csv\n",
      "Transforming us_08-02-2020.csv\n",
      "Transforming us_08-03-2020.csv\n",
      "Transforming us_11-21-2020.csv\n",
      "Transforming us_11-20-2020.csv\n",
      "Transforming us_10-23-2020.csv\n",
      "Transforming us_10-22-2020.csv\n",
      "Transforming us_07-19-2020.csv\n",
      "Transforming us_07-18-2020.csv\n",
      "Transforming us_09-01-2020.csv\n",
      "Transforming us_06-25-2020.csv\n",
      "Transforming us_06-24-2020.csv\n",
      "Transforming us_05-16-2020.csv\n",
      "Transforming us_05-17-2020.csv\n"
     ]
    }
   ],
   "source": [
    "# remap headers to consistent format\n",
    "\n",
    "files = [f for f in os.listdir(TRANSFORMED_DATA_DIR) if f.startswith('us_')]\n",
    "for f in files:\n",
    "    fname, fext = os.path.splitext(f)\n",
    "    date_str = fname.replace('us_', '')\n",
    "    file_path = os.path.join(TRANSFORMED_DATA_DIR, f)\n",
    "    with open(file_path) as fp:\n",
    "        headers = fp.readline().strip()\n",
    "        df = pd.read_csv(file_path)\n",
    "        if headers not in transforms.known_headers:\n",
    "            print(\"{} has unrecognized headers {}\".format(f, headers))\n",
    "            df.head()\n",
    "            sys.exit(1)\n",
    "\n",
    "        print('Transforming {}'.format(f))\n",
    "        \n",
    "        transformed_df = transforms.transform_headers(df, date_str)\n",
    "    transformed_path = os.path.join(TRANSFORMED_DATA_DIR, 'transformed_'+date_str+'.csv')\n",
    "\n",
    "    if os.path.exists(transformed_path):\n",
    "        global_df = pd.read_csv(transformed_path)\n",
    "        for country in transformed_df.country.unique():\n",
    "            global_df = global_df.loc[global_df.country != country]\n",
    "        transformed_df = pd.concat([transformed_df, global_df])\n",
    "\n",
    "    transformed_df.to_csv(transformed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  date  city           state country  latitude  longitude  \\\n",
       "0  2020-05-17 00:00:00   NaN         Alabama      US   32.3182   -86.9023   \n",
       "1  2020-05-17 00:00:00   NaN          Alaska      US   61.3707  -152.4044   \n",
       "2  2020-05-17 00:00:00   NaN  American Samoa      US  -14.2710  -170.1320   \n",
       "3  2020-05-17 00:00:00   NaN         Arizona      US   33.7298  -111.4312   \n",
       "4  2020-05-17 00:00:00   NaN        Arkansas      US   34.9697   -92.3731   \n",
       "\n",
       "   cases  deaths  recoveries  testing_rate  hospitalization_rate  cases_100K  \\\n",
       "0  12137     488         NaN   3188.743643             11.825673  240.068445   \n",
       "1    388      10       344.0   4736.687422                   NaN   53.038432   \n",
       "2      0       0         NaN    188.709764                   NaN    0.000000   \n",
       "3  13945     680      3450.0   2084.996573             12.183578  191.585962   \n",
       "4   4759      98      3590.0   2822.747932             10.926665  157.697452   \n",
       "\n",
       "         combined_key  FIPS         UID  Unnamed: 0  \n",
       "0         alabama, us   1.0  84000001.0         NaN  \n",
       "1          alaska, us   2.0  84000002.0         NaN  \n",
       "2  american samoa, us  60.0        16.0         NaN  \n",
       "3         arizona, us   4.0  84000004.0         NaN  \n",
       "4        arkansas, us   5.0  84000005.0         NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>city</th>\n      <th>state</th>\n      <th>country</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>cases</th>\n      <th>deaths</th>\n      <th>recoveries</th>\n      <th>testing_rate</th>\n      <th>hospitalization_rate</th>\n      <th>cases_100K</th>\n      <th>combined_key</th>\n      <th>FIPS</th>\n      <th>UID</th>\n      <th>Unnamed: 0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-05-17 00:00:00</td>\n      <td>NaN</td>\n      <td>Alabama</td>\n      <td>US</td>\n      <td>32.3182</td>\n      <td>-86.9023</td>\n      <td>12137</td>\n      <td>488</td>\n      <td>NaN</td>\n      <td>3188.743643</td>\n      <td>11.825673</td>\n      <td>240.068445</td>\n      <td>alabama, us</td>\n      <td>1.0</td>\n      <td>84000001.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-05-17 00:00:00</td>\n      <td>NaN</td>\n      <td>Alaska</td>\n      <td>US</td>\n      <td>61.3707</td>\n      <td>-152.4044</td>\n      <td>388</td>\n      <td>10</td>\n      <td>344.0</td>\n      <td>4736.687422</td>\n      <td>NaN</td>\n      <td>53.038432</td>\n      <td>alaska, us</td>\n      <td>2.0</td>\n      <td>84000002.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-05-17 00:00:00</td>\n      <td>NaN</td>\n      <td>American Samoa</td>\n      <td>US</td>\n      <td>-14.2710</td>\n      <td>-170.1320</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>188.709764</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>american samoa, us</td>\n      <td>60.0</td>\n      <td>16.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-05-17 00:00:00</td>\n      <td>NaN</td>\n      <td>Arizona</td>\n      <td>US</td>\n      <td>33.7298</td>\n      <td>-111.4312</td>\n      <td>13945</td>\n      <td>680</td>\n      <td>3450.0</td>\n      <td>2084.996573</td>\n      <td>12.183578</td>\n      <td>191.585962</td>\n      <td>arizona, us</td>\n      <td>4.0</td>\n      <td>84000004.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-05-17 00:00:00</td>\n      <td>NaN</td>\n      <td>Arkansas</td>\n      <td>US</td>\n      <td>34.9697</td>\n      <td>-92.3731</td>\n      <td>4759</td>\n      <td>98</td>\n      <td>3590.0</td>\n      <td>2822.747932</td>\n      <td>10.926665</td>\n      <td>157.697452</td>\n      <td>arkansas, us</td>\n      <td>5.0</td>\n      <td>84000005.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "US\nItaly\nCanada\nSpain\nUnited Kingdom\nChina\nNetherlands\nAustralia\nGermany\nDenmark\nFrance\nAfghanistan\nAlbania\nAlgeria\nAndorra\nAngola\nAntigua and Barbuda\nArgentina\nArmenia\nAustria\nAzerbaijan\nBahamas\nBahrain\nBangladesh\nBarbados\nBelarus\nBelgium\nBelize\nBenin\nBhutan\nBolivia\nBosnia and Herzegovina\nBotswana\nBrazil\nBrunei\nBulgaria\nBurkina Faso\nBurma\nBurundi\nCabo Verde\nCambodia\nCameroon\nCentral African Republic\nChad\nChile\nColombia\nComoros\nCongo (Brazzaville)\nCongo (Kinshasa)\nCosta Rica\nCote d'Ivoire\nCroatia\nCuba\nCyprus\nCzechia\nDiamond Princess\nDjibouti\nDominica\nDominican Republic\nEcuador\nEgypt\nEl Salvador\nEquatorial Guinea\nEritrea\nEstonia\nEswatini\nEthiopia\nFiji\nFinland\nGabon\nGambia\nGeorgia\nGhana\nGreece\nGrenada\nGuatemala\nGuinea\nGuinea-Bissau\nGuyana\nHaiti\nHoly See\nHonduras\nHungary\nIceland\nIndia\nIndonesia\nIran\nIraq\nIreland\nIsrael\nJamaica\nJapan\nJordan\nKazakhstan\nKenya\nKorea, South\nKosovo\nKuwait\nKyrgyzstan\nLaos\nLatvia\nLebanon\nLesotho\nLiberia\nLibya\nLiechtenstein\nLithuania\nLuxembourg\nMS Zaandam\nMadagascar\nMalawi\nMalaysia\nMaldives\nMali\nMalta\nMauritania\nMauritius\nMexico\nMoldova\nMonaco\nMongolia\nMontenegro\nMorocco\nMozambique\nNamibia\nNepal\nNew Zealand\nNicaragua\nNiger\nNigeria\nNorth Macedonia\nNorway\nOman\nPakistan\nPanama\nPapua New Guinea\nParaguay\nPeru\nPhilippines\nPoland\nPortugal\nQatar\nRomania\nRussia\nRwanda\nSaint Kitts and Nevis\nSaint Lucia\nSaint Vincent and the Grenadines\nSan Marino\nSao Tome and Principe\nSaudi Arabia\nSenegal\nSerbia\nSeychelles\nSierra Leone\nSingapore\nSlovakia\nSlovenia\nSomalia\nSouth Africa\nSouth Sudan\nSri Lanka\nSudan\nSuriname\nSweden\nSwitzerland\nSyria\nTaiwan*\nTajikistan\nTanzania\nThailand\nTimor-Leste\nTogo\nTrinidad and Tobago\nTunisia\nTurkey\nUganda\nUkraine\nUnited Arab Emirates\nUruguay\nUzbekistan\nVenezuela\nVietnam\nWest Bank and Gaza\nWestern Sahara\nYemen\nZambia\nZimbabwe\n"
     ]
    }
   ],
   "source": [
    "for country in transformed_df.country.unique():\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}